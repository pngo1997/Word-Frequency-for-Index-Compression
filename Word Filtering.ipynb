{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5115877-70a8-4b46-a1cc-c695ca471e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ba9c-2561-474a-b649-16db3404f0a0",
   "metadata": {},
   "source": [
    "### Import dataset as pandas dataframe. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6244b9-0ba7-4cd3-aecd-5881d90cffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numnber of rows and columns of the ted data: (2550, 17)\n"
     ]
    }
   ],
   "source": [
    "ted = pd.read_csv('ted_main.csv', encoding='utf-8')\n",
    "print(f'Numnber of rows and columns of the ted data: {ted.shape}')\n",
    "#Read data as Pandas dataframe and get number of rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3771302a-1ba8-4bbe-a629-db754306ec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "\n",
       "     event   film_date  languages  main_speaker  \\\n",
       "0  TED2006  1140825600         60  Ken Robinson   \n",
       "1  TED2006  1140825600         43       Al Gore   \n",
       "2  TED2006  1140739200         26   David Pogue   \n",
       "\n",
       "                                        name  num_speaker  published_date  \\\n",
       "0  Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1       Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2              David Pogue: Simplicity sells            1      1151367060   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "\n",
       "                                       related_talks    speaker_occupation  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...       Author/educator   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...      Climate advocate   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...  Technology columnist   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "\n",
       "                         title  \\\n",
       "0  Do schools kill creativity?   \n",
       "1  Averting the climate crisis   \n",
       "2             Simplicity sells   \n",
       "\n",
       "                                                 url     views  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head(3)\n",
    "#Get the first 3 rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a3174-e3b3-48f5-aacf-239ac4a42d59",
   "metadata": {},
   "source": [
    "### Extract 'description' 2nd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1cf745-23e6-40d7-aaf2-e0b915b02c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row of the description column: (2550,)\n"
     ]
    }
   ],
   "source": [
    "tedDesc = ted['description']\n",
    "print(f'Number of row of the description column: {tedDesc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3095791-8311-4b6f-bb37-b51b55dd8b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With the same humor and humanity he exuded in \"An Inconvenient Truth,\" Al Gore spells out 15 ways that individuals can address climate change immediately, from buying a hybrid to inventing a new, hotter brand name for global warming.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tedDesc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd7140-ad47-476d-977f-94f0c63bf2f0",
   "metadata": {},
   "source": [
    "### [A] Word tokenization (only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524afb1-dcfa-4770-9f73-556298c3b648",
   "metadata": {},
   "source": [
    "#### (1) Total # of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bce2b04-5eb2-4d03-910b-8c293936cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tokens - word tokenization only: 152000\n"
     ]
    }
   ],
   "source": [
    "#1st loop: For each description in tedDesc, apply word tokenization. desc is a list.\n",
    "#2nd loop: For each token in desc list, extend it to the vocabList1 token accumulator.\n",
    "vocabList1 = [token for desc in tedDesc.apply(word_tokenize) for token in desc]\n",
    "\n",
    "print(f'Total # of tokens - word tokenization only: {len(vocabList1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271ea53-789e-4ed2-b78c-48091509cbde",
   "metadata": {},
   "source": [
    "#### (2) Size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cafb632-c858-42d9-9394-b4ec6f651cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary - word tokenization only: 17878\n"
     ]
    }
   ],
   "source": [
    "#Count frequencies of the vocabulary terms.\n",
    "vocabDict1 = nltk.FreqDist(vocabList1)\n",
    "\n",
    "#Get unique vocabulary.\n",
    "uniqueVocab1 = list(vocabDict1.keys())\n",
    "\n",
    "#Number of unique tokens.\n",
    "print(f'Size of vocabulary - word tokenization only: {len(uniqueVocab1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7e78a-b67e-47ab-b58f-771c97a5ca52",
   "metadata": {},
   "source": [
    "#### (3) Top 30 most common token types with frequency (list in descending order of frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5174cad9-f629-423b-9d65-5ea1daf43e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common token types - word tokenization only:\n",
      "[(',', 7382), ('.', 5764), ('the', 5395), ('and', 4264), ('of', 3651), ('to', 3528), ('a', 3505), ('in', 1762), ('--', 1485), ('that', 1472), (\"'s\", 1217), ('for', 1140), ('``', 898), (\"''\", 893), ('with', 879), ('we', 878), ('is', 834), ('it', 833), ('?', 824), ('this', 812), ('In', 762), ('on', 761), ('how', 746), ('he', 738), ('from', 707), ('talk', 693), ('his', 686), (':', 643), ('about', 630), ('as', 605)]\n"
     ]
    }
   ],
   "source": [
    "top30Common1 = vocabDict1.most_common(30)\n",
    "\n",
    "print('Top 30 most common token types - word tokenization only:')\n",
    "print(top30Common1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a5391-e07c-48a7-b16e-e569b98f45f7",
   "metadata": {},
   "source": [
    "#### Write output to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d11b3e4-7285-403c-bf3f-03237c17c6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written!\n"
     ]
    }
   ],
   "source": [
    "with open('top30-PartA.csv', 'w', newline='', encoding='utf-8') as outFile:\n",
    "    csvWriter = csv.writer(outFile)\n",
    "    #Header\n",
    "    csvWriter.writerow([\"Token\", \"Frequency\"])\n",
    "    #Data\n",
    "    csvWriter.writerows(top30Common1)\n",
    "\n",
    "print(f\"Successfully written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caac9b-721c-47a3-a56f-aa298721fa8b",
   "metadata": {},
   "source": [
    "#### (4) Percentage of tokens in the dataset that is covered by the top 30 token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18416244-1ef4-44a9-a67e-ec62ee211444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens for top 30: 54387\n",
      "Percentage of tokens covered by the top 30 token types - word tokenization only: 35.781%\n"
     ]
    }
   ],
   "source": [
    "#Get total number of tokens of the top 30. \n",
    "totalTokens1_top30 = sum(frequency for token, frequency in top30Common1)\n",
    "print(f'Total number of tokens for top 30: {totalTokens1_top30}')\n",
    "\n",
    "#Percentage covered by top 30.\n",
    "top30Perc1 = (totalTokens1_top30 / len(vocabList1)) * 100\n",
    "print(f'Percentage of tokens covered by the top 30 token types - word tokenization only: {top30Perc1:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c20221-4088-4498-86ed-ce50c02e6154",
   "metadata": {},
   "source": [
    "#### Breakdown chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9eecf01-3300-46de-b6a6-9029ad2b31e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>4.856579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>3.792105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>3.549342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>2.805263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>2.401974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>2.321053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>2.305921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>1.159211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>--</td>\n",
       "      <td>0.976974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>0.968421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'s</td>\n",
       "      <td>0.800658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>``</td>\n",
       "      <td>0.590789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>''</td>\n",
       "      <td>0.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>0.578289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>we</td>\n",
       "      <td>0.577632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>is</td>\n",
       "      <td>0.548684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>it</td>\n",
       "      <td>0.548026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>?</td>\n",
       "      <td>0.542105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>this</td>\n",
       "      <td>0.534211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>In</td>\n",
       "      <td>0.501316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>on</td>\n",
       "      <td>0.500658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how</td>\n",
       "      <td>0.490789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>he</td>\n",
       "      <td>0.485526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>from</td>\n",
       "      <td>0.465132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>talk</td>\n",
       "      <td>0.455921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>his</td>\n",
       "      <td>0.451316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>:</td>\n",
       "      <td>0.423026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>about</td>\n",
       "      <td>0.414474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>as</td>\n",
       "      <td>0.398026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token  Percentage\n",
       "0       ,    4.856579\n",
       "1       .    3.792105\n",
       "2     the    3.549342\n",
       "3     and    2.805263\n",
       "4      of    2.401974\n",
       "5      to    2.321053\n",
       "6       a    2.305921\n",
       "7      in    1.159211\n",
       "8      --    0.976974\n",
       "9    that    0.968421\n",
       "10     's    0.800658\n",
       "11    for    0.750000\n",
       "12     ``    0.590789\n",
       "13     ''    0.587500\n",
       "14   with    0.578289\n",
       "15     we    0.577632\n",
       "16     is    0.548684\n",
       "17     it    0.548026\n",
       "18      ?    0.542105\n",
       "19   this    0.534211\n",
       "20     In    0.501316\n",
       "21     on    0.500658\n",
       "22    how    0.490789\n",
       "23     he    0.485526\n",
       "24   from    0.465132\n",
       "25   talk    0.455921\n",
       "26    his    0.451316\n",
       "27      :    0.423026\n",
       "28  about    0.414474\n",
       "29     as    0.398026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30tokenPerc1 = [(token, (frequency / len(vocabList1)) * 100) for token, frequency in top30Common1]\n",
    "\n",
    "#Convert to pandas data frame.\n",
    "df_top30tokenPerc1 = pd.DataFrame(top30tokenPerc1, columns=['Token', 'Percentage'])\n",
    "df_top30tokenPerc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f70d5-75b7-49b3-83a9-fb0929312d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f548400-cc70-46fa-9746-195310869fdb",
   "metadata": {},
   "source": [
    "### [B] Word tokenization + Case folding (lower-case) + Stopword filtering + Non-alphabet filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b5d88-d615-43fd-bfe9-a8e90d08cfa9",
   "metadata": {},
   "source": [
    "#### (1) Total # of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2509335d-1160-493d-9332-bc64f19d529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tokens - tokenization/lower-case/non stop-word/alphabet: 74131\n"
     ]
    }
   ],
   "source": [
    "#Set to store English stop word for efficiency.\n",
    "stopwordsSet = set(stopwords.words('english'))\n",
    "\n",
    "#1st loop: For each description in tedDesc, apply case folding & word tokenization. desc is a list.\n",
    "#2nd loop: For each token in desc list, filtering if the token is non-alphabet and not a stop-word.\n",
    "vocabList2 = [token for desc in tedDesc for token in word_tokenize(desc.lower())\n",
    "    if token.isalpha() and token not in stopwordsSet]\n",
    "\n",
    "print(f'Total # of tokens - tokenization/lower-case/non stop-word/alphabet: {len(vocabList2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe957d8-b8c0-4ecd-aba2-a6261e4bdc7f",
   "metadata": {},
   "source": [
    "#### (2) Size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c13e52b9-a771-443f-aabe-79ea1782de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary - tokenization/lower-case/non stop-word/alphabet: 14714\n"
     ]
    }
   ],
   "source": [
    "#Count frequencies of the vocabulary terms.\n",
    "vocabDict2 = nltk.FreqDist(vocabList2)\n",
    "\n",
    "#Get unique vocabulary.\n",
    "uniqueVocab2 = list(vocabDict2.keys())\n",
    "\n",
    "#Number of unique tokens.\n",
    "print(f'Size of vocabulary - tokenization/lower-case/non stop-word/alphabet: {len(uniqueVocab2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c75e3-0af3-4cf2-9ac4-cacce2589b2d",
   "metadata": {},
   "source": [
    "#### (3) Top 30 most common token types with frequency (list in descending order of frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf1cf6c-95e6-4129-8ae4-22b691504be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common token types - tokenization/lower-case/non stop-word/alphabet:\n",
      "[('talk', 700), ('us', 643), ('world', 515), ('new', 415), ('says', 411), ('people', 332), ('shares', 326), ('shows', 282), ('life', 274), ('one', 272), ('ted', 254), ('like', 251), ('make', 239), ('way', 227), ('human', 205), ('work', 203), ('could', 200), ('help', 184), ('even', 179), ('story', 179), ('time', 168), ('years', 163), ('makes', 153), ('talks', 148), ('data', 142), ('future', 142), ('change', 140), ('powerful', 139), ('know', 133), ('two', 130)]\n"
     ]
    }
   ],
   "source": [
    "top30Common2 = vocabDict2.most_common(30)\n",
    "\n",
    "print('Top 30 most common token types - tokenization/lower-case/non stop-word/alphabet:')\n",
    "print(top30Common2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13b45c-b42f-4362-9ebd-1c1e7351a8d7",
   "metadata": {},
   "source": [
    "#### Write output to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b58c8f43-8aec-40b2-93c2-0d7b3441c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written!\n"
     ]
    }
   ],
   "source": [
    "with open('top30-PartB.csv', 'w', newline='', encoding='utf-8') as outFile:\n",
    "    csvWriter = csv.writer(outFile)\n",
    "    #Header\n",
    "    csvWriter.writerow([\"Token\", \"Frequency\"])\n",
    "    #Data\n",
    "    csvWriter.writerows(top30Common2)\n",
    "\n",
    "print(f\"Successfully written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b6bef-56ae-4fb7-bbc4-589a80d46129",
   "metadata": {},
   "source": [
    "#### (4) Percentage of tokens in the dataset that is covered by the top 30 token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a97f9deb-67b8-4bec-9fff-bd131bef3860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens for top 30: 7749\n",
      "Percentage of tokens covered by the top 30 token types - tokenization/lower-case/non stop-word/alphabet: 10.453%\n"
     ]
    }
   ],
   "source": [
    "#Get total number of tokens of the top 30. \n",
    "totalTokens2_top30 = sum(frequency for token, frequency in top30Common2)\n",
    "print(f'Total number of tokens for top 30: {totalTokens2_top30}')\n",
    "\n",
    "#Percentage covered by top 30.\n",
    "top30Perc2 = (totalTokens2_top30 / len(vocabList2)) * 100\n",
    "print(f'Percentage of tokens covered by the top 30 token types - tokenization/lower-case/non stop-word/alphabet: {top30Perc2:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef457e-d47a-42ea-b150-04ceb88c2227",
   "metadata": {},
   "source": [
    "#### Breakdown chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40d7459a-5b7d-458f-b400-bc30428f51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talk</td>\n",
       "      <td>0.944274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us</td>\n",
       "      <td>0.867383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>0.694716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>0.559820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>says</td>\n",
       "      <td>0.554424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>people</td>\n",
       "      <td>0.447856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shares</td>\n",
       "      <td>0.439762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shows</td>\n",
       "      <td>0.380408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>life</td>\n",
       "      <td>0.369616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>one</td>\n",
       "      <td>0.366918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ted</td>\n",
       "      <td>0.342637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>0.338590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>make</td>\n",
       "      <td>0.322402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>way</td>\n",
       "      <td>0.306215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>human</td>\n",
       "      <td>0.276537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>work</td>\n",
       "      <td>0.273840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>could</td>\n",
       "      <td>0.269793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>help</td>\n",
       "      <td>0.248209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>even</td>\n",
       "      <td>0.241464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>story</td>\n",
       "      <td>0.241464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>time</td>\n",
       "      <td>0.226626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>years</td>\n",
       "      <td>0.219881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>makes</td>\n",
       "      <td>0.206391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>talks</td>\n",
       "      <td>0.199647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>data</td>\n",
       "      <td>0.191553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>future</td>\n",
       "      <td>0.191553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>change</td>\n",
       "      <td>0.188855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>powerful</td>\n",
       "      <td>0.187506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>know</td>\n",
       "      <td>0.179412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>two</td>\n",
       "      <td>0.175365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Token  Percentage\n",
       "0       talk    0.944274\n",
       "1         us    0.867383\n",
       "2      world    0.694716\n",
       "3        new    0.559820\n",
       "4       says    0.554424\n",
       "5     people    0.447856\n",
       "6     shares    0.439762\n",
       "7      shows    0.380408\n",
       "8       life    0.369616\n",
       "9        one    0.366918\n",
       "10       ted    0.342637\n",
       "11      like    0.338590\n",
       "12      make    0.322402\n",
       "13       way    0.306215\n",
       "14     human    0.276537\n",
       "15      work    0.273840\n",
       "16     could    0.269793\n",
       "17      help    0.248209\n",
       "18      even    0.241464\n",
       "19     story    0.241464\n",
       "20      time    0.226626\n",
       "21     years    0.219881\n",
       "22     makes    0.206391\n",
       "23     talks    0.199647\n",
       "24      data    0.191553\n",
       "25    future    0.191553\n",
       "26    change    0.188855\n",
       "27  powerful    0.187506\n",
       "28      know    0.179412\n",
       "29       two    0.175365"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30tokenPerc2 = [(token, (frequency / len(vocabList2)) * 100) for token, frequency in top30Common2]\n",
    "\n",
    "#Convert to pandas data frame.\n",
    "df_top30tokenPerc2 = pd.DataFrame(top30tokenPerc2, columns=['Token', 'Percentage'])\n",
    "df_top30tokenPerc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc80f3-5295-4bd5-b747-7036a8642466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e9ee3c1-72fb-4477-a7ef-d162081f502a",
   "metadata": {},
   "source": [
    "### [C] Word tokenization + Case folding (lower-case) + Stopword filtering + Non-alphabet filtering + Porter stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab65d88-6e3c-4e57-a1ce-57ae508e7602",
   "metadata": {},
   "source": [
    "#### (1) Total # of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ff5b36-c67c-4328-99d2-828e51ad1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tokens - tokenization/lower-case/non stop-word/alphabet/stemmed: 74131\n"
     ]
    }
   ],
   "source": [
    "#Set to store English stop word for efficiency.\n",
    "stopwordsSet = set(stopwords.words('english'))\n",
    "\n",
    "#Porter stemmer object.\n",
    "porterStemmer = nltk.PorterStemmer()\n",
    "\n",
    "#1st loop: For each description in tedDesc, apply case folding & word tokenization. desc is a list.\n",
    "#2nd loop: For each token in desc list, filtering if the token is non-alphabet and not a stop-word.\n",
    "vocabList3 = [porterStemmer.stem(token) for desc in tedDesc for token in word_tokenize(desc.lower())\n",
    "    if token.isalpha() and token not in stopwordsSet]\n",
    "\n",
    "print(f'Total # of tokens - tokenization/lower-case/non stop-word/alphabet/stemmed: {len(vocabList3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fbcd1-ada0-4440-91ba-d9530e46e8ef",
   "metadata": {},
   "source": [
    "#### (2) Size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83850980-aa49-46c4-ac6e-fce9d97292d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary - tokenization/lower-case/non stop-word/alphabet/stemmed: 10592\n"
     ]
    }
   ],
   "source": [
    "#Count frequencies of the vocabulary terms.\n",
    "vocabDict3 = nltk.FreqDist(vocabList3)\n",
    "\n",
    "#Get unique vocabulary.\n",
    "uniqueVocab3 = list(vocabDict3.keys())\n",
    "\n",
    "#Number of unique tokens.\n",
    "print(f'Size of vocabulary - tokenization/lower-case/non stop-word/alphabet/stemmed: {len(uniqueVocab3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e1155-b44d-48dd-83e6-eb4c07933a53",
   "metadata": {},
   "source": [
    "#### (3) Top 30 most common token types with frequency (list in descending order of frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe8c328-8a26-46c3-b5e8-94a15dcc0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common token types - tokenization/lower-case/non stop-word/alphabet/stemmed:\n",
      "[('talk', 880), ('us', 643), ('world', 527), ('say', 453), ('make', 449), ('share', 444), ('new', 415), ('show', 371), ('use', 360), ('work', 356), ('peopl', 334), ('human', 330), ('way', 326), ('one', 307), ('stori', 307), ('live', 282), ('help', 281), ('life', 274), ('like', 272), ('power', 262), ('ted', 254), ('design', 240), ('take', 223), ('learn', 221), ('look', 219), ('time', 213), ('year', 210), ('think', 204), ('creat', 203), ('could', 200)]\n"
     ]
    }
   ],
   "source": [
    "top30Common3 = vocabDict3.most_common(30)\n",
    "\n",
    "print('Top 30 most common token types - tokenization/lower-case/non stop-word/alphabet/stemmed:')\n",
    "print(top30Common3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d075a8-7594-4683-90bd-12ba8488e639",
   "metadata": {},
   "source": [
    "#### Write output to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fee4c68-03c8-40ca-bf1e-9ec5aa8e1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written!\n"
     ]
    }
   ],
   "source": [
    "with open('top30-PartC.csv', 'w', newline='', encoding='utf-8') as outFile:\n",
    "    csvWriter = csv.writer(outFile)\n",
    "    #Header\n",
    "    csvWriter.writerow([\"Token\", \"Frequency\"])\n",
    "    #Data\n",
    "    csvWriter.writerows(top30Common3)\n",
    "\n",
    "print(f\"Successfully written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a186ef-eb97-4753-a86c-a8c6570e6d8f",
   "metadata": {},
   "source": [
    "#### (4) Percentage of tokens in the dataset that is covered by the top 30 token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762f97b2-f1ee-4948-82fe-e3c81c1e6ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens for top 30: 10060\n",
      "Percentage of tokens covered by the top 30 token types - tokenization/lower-case/non stop-word/alphabet/stemmed: 13.571%\n"
     ]
    }
   ],
   "source": [
    "#Get total number of tokens of the top 30. \n",
    "totalTokens3_top30 = sum(frequency for token, frequency in top30Common3)\n",
    "print(f'Total number of tokens for top 30: {totalTokens3_top30}')\n",
    "\n",
    "#Percentage covered by top 30.\n",
    "top30Perc3 = (totalTokens3_top30 / len(vocabList3)) * 100\n",
    "print(f'Percentage of tokens covered by the top 30 token types - tokenization/lower-case/non stop-word/alphabet/stemmed: {top30Perc3:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9ecae-2bbd-4d7e-9746-928b2c69e1b7",
   "metadata": {},
   "source": [
    "#### Breakdown chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4921244f-1a6a-45aa-9c15-69256ffec48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talk</td>\n",
       "      <td>1.187088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us</td>\n",
       "      <td>0.867383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>0.710904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>say</td>\n",
       "      <td>0.611080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>make</td>\n",
       "      <td>0.605685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>share</td>\n",
       "      <td>0.598940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new</td>\n",
       "      <td>0.559820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show</td>\n",
       "      <td>0.500465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>use</td>\n",
       "      <td>0.485627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>work</td>\n",
       "      <td>0.480231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>peopl</td>\n",
       "      <td>0.450554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>human</td>\n",
       "      <td>0.445158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td>0.439762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>0.414132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>stori</td>\n",
       "      <td>0.414132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>live</td>\n",
       "      <td>0.380408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>help</td>\n",
       "      <td>0.379059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>life</td>\n",
       "      <td>0.369616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>like</td>\n",
       "      <td>0.366918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>power</td>\n",
       "      <td>0.353428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ted</td>\n",
       "      <td>0.342637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>design</td>\n",
       "      <td>0.323751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>take</td>\n",
       "      <td>0.300819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>learn</td>\n",
       "      <td>0.298121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>look</td>\n",
       "      <td>0.295423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>time</td>\n",
       "      <td>0.287329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>year</td>\n",
       "      <td>0.283282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>think</td>\n",
       "      <td>0.275189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>creat</td>\n",
       "      <td>0.273840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>could</td>\n",
       "      <td>0.269793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token  Percentage\n",
       "0     talk    1.187088\n",
       "1       us    0.867383\n",
       "2    world    0.710904\n",
       "3      say    0.611080\n",
       "4     make    0.605685\n",
       "5    share    0.598940\n",
       "6      new    0.559820\n",
       "7     show    0.500465\n",
       "8      use    0.485627\n",
       "9     work    0.480231\n",
       "10   peopl    0.450554\n",
       "11   human    0.445158\n",
       "12     way    0.439762\n",
       "13     one    0.414132\n",
       "14   stori    0.414132\n",
       "15    live    0.380408\n",
       "16    help    0.379059\n",
       "17    life    0.369616\n",
       "18    like    0.366918\n",
       "19   power    0.353428\n",
       "20     ted    0.342637\n",
       "21  design    0.323751\n",
       "22    take    0.300819\n",
       "23   learn    0.298121\n",
       "24    look    0.295423\n",
       "25    time    0.287329\n",
       "26    year    0.283282\n",
       "27   think    0.275189\n",
       "28   creat    0.273840\n",
       "29   could    0.269793"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30tokenPerc3 = [(token, (frequency / len(vocabList3)) * 100) for token, frequency in top30Common3]\n",
    "\n",
    "#Convert to pandas data frame.\n",
    "df_top30tokenPerc3 = pd.DataFrame(top30tokenPerc3, columns=['Token', 'Percentage'])\n",
    "df_top30tokenPerc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1a3ec-244f-4acb-a5ef-60099755cf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
